## Documentation

The Postgres Performance Farm is a system in charge of running benchmarking tests through different Postgres versions. NOTE: this documentation and folder are only relevant for the client script. For the API and website, refer to the pgperffarm_server repository.

Each benchmark aims to run on a newly installed Postgres, without major changes in the default configurations; however, it is possible to change basic parameters. The install does not depend nor interfere with previously existing installations: all directories and sockets are separate by default.

The currently available test is PgBench, also configurable. Furthermore, the script collects basic system information, such as hardware and operating system. There is also execution of collectd, if available.

##### Terminology

The main assumption of the script is that the most relevant results are collecting running tests through different commits within the Postgres repository. Having the same test ran multiple times without any changes in hardware or version does not provide useful information.

Commonly used words are:

* A **run** is an execution of the script on a single client;
* A **benchmark** is a single execution of PgBench;
* A given run can have multiple benchmarks done;
* A run should have at least one benchmark.

##### Output

The client produces the following output:

* JSON results with output of PgBench and system information;
* Logs per-interval generated by PgBench, with intervals of 1 second;
* Error and operations logs.



### Client script

The client script is written in Python 3 and takes care of all the client-side part, i. e. downloading and installing Postgres, starting a cluster, executing tests, collecting system information and then shutting down processes and deleting unnecessary folders.

The workflow can be summarized as:

1. Creating (or recreating) all necessary directories;
2. Checking for existing Postgres clones within the Performance Farm folders, and if so, trying to pull for updates;
3. Saving current branch and commit;
4. If there has been an update, or a clone from scratch, Postgres is built and reinstalled using `make`;
5. User-defined parameters are being checked to see whether there are errors;
6. A cluster is initialized and started through `pg_ctl`;
7. System information is collected;
8. PgBench is executed:
   1. For each client, initdb is performed;
   2. Tests are run;
9. Cluster is stopped;
10. Data directory is cleaned;
11. Logs are being attached to results and shipped to the server;
12. The script then restarts at step 1 with a different branch, until branches of interests are terminated.

##### PgBench parameters:

* Clients (number of threads);
* Scale of the database;
* Duration of each execution of PgBench;
* Iterations, number of executions. 

Furthermore, PgBench is ran with additional parameters, such as display of statement latencies and logging of interval every second.

##### Files and folders

* perffarm-client.py: takes care of coordinating the setup of benchmarks, and when everything is initialized runs collectors;
* folders.py: global variables related to default folder structure;
* path.py: initialisation of folders for each branch;
* branches.py: list of all branches to use for running tests;
* Benchmarks:
  * pgbench.py: takes care of all PgBench related tasks, such as initialization, run of benchmarks and collecting results;
  * runner.py: wrapper calling PgBench functions and saving output in a file
* Collectors:
  * collectd.py: runs collectd to gather system and database statistics;
  * collector.py: combines other collectors and calls them;
  * system.py: contains a collection of Python3 functions from external modules to extract system information such as CPU usage, kernel configuration and memory;
  * postgres.py: mainly a function that connects to Postgres and selects its settings;
* Post example (removed in later versions): 
  * upload.py: takes the output file and sends it to the API;
* Utils: 
  * cluster.py: initalizes, starts and stops a Postgres cluster;
  * build.py: module which takes care of executing a build from source from a git repository;
  * locking.py: ensures locking of files;
  * logging.py: prints nice logging;
  * misc.py: connects to database and returns available RAM.

##### Tuning settings

The Performance Farm comes with some default settings as well as local settings that can be adjusted. Parameters that can be changed are:

* UPDATE, flag to instruct the script to try to pull for new commits at every execution;
* AUTOMATIC_UPLOAD, flag to enable uploading to the server (works only with a valid machine secret);
* GIT_URL, used to specify the repository to pull from;
* BASE_PATH, folder which will be used for Performance Farm files;
* API_URL, current URL of the server (can be changed for local developing);
* MACHINE_SECRET, identifier of the machine on which benchmarks are being run (generated by the server);
* POSTGRES_CONFIG, basic Postgres parameters;
* DATABASE_NAME, database name;
* PGBENCH_CONFIG, parameters to tune PgBench.

##### Folder structure

All folders used within the Performance Farm are children of BASE_PATH, specified in the settings. Specifically, there are:

* BUILD_PATH, containing files generated by configure;
* INSTALL_PATH, installation directory of make;
* BIN_PATH, bin directory of the Postgres installation;
* OUTPUT_PATH, containing all output files generated by PgBench and the script;
* REPOSITORY_PATH, clone of the Postgres remote in which updates are checked;
* DATADIR_PATH, Postgres data directory, getting removed after every execution;
* SOCKET_PATH, folder for sockets to avoid interfering with other Postgres processes;
* LOG_PATH, containing all logs, errors and messages generated by the script.

##### Interpreting results

Output of the script is saved in a JSON format to take advantage of key-value storage, useful for collecting and parsed.

Results are contained in the output folder. There are two main output files: results.json, containing actual results, and results_complete.json, which is the file getting shipped to the server and is equal to the results file plus all the logs attached in sequence. It lacks human readability, therefore the file which should be checked is the first. 

Main fields of the JSON output are:

* pgbench, with a number of nested objects corresponding to the number of runs, each of them with basic information about latency and tps;

* Linux information, divided in:

  * CPU data, along with system times;
  * OS information, its version and architecture;
  * Memory, containing virtual, swap, mounts;
  * Disk usage with I/O;
  * Process information;
  * Compilers (make and gcc);
  * Collectd results;
  * Postgres configuraton;
  * Meta information (date, time, user name).

  

